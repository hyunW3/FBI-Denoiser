{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\"../../\"))\n",
    "sys.path.append(os.path.abspath(\"../core\"))\n",
    "sys.path.append(\"/mnt/ssd/hyun/experiment/GAN_n2n/core\")\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from edge_core.model import Generator\n",
    "from core.median_filter import *\n",
    "from core.median_filter import apply_median_filter_cpu\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "img_dict = {}\n",
    "with h5py.File(\"../../data/test_Samsung_SNU_patches_SET050607080910_divided_by_fnum_setnum.hdf5\") as f:\n",
    "    for key in f.keys():\n",
    "        #print(key)\n",
    "        img_dict[key] = {} # f[key]\n",
    "        for subkey in f[key].keys():\n",
    "            img_dict[key][subkey] = np.array(f[key][subkey])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from core.utils import TrdataLoader, TedataLoader\n",
    "from arguments import get_args\n",
    "from torch.utils.data import DataLoader\n",
    "args = get_args(env='colab')\n",
    "args.pge_weight_dir = None #pge_weight_dir\n",
    "args.loss_function = \"MSE_Affine\"\n",
    "args.noise_type = 'Poisson-Gaussian'\n",
    "args.model_type = 'FBI_Net'\n",
    "# args.set_num = '1'\n",
    "args.data_name = 'Samsung'\n",
    "args.data_type = 'Grayscale'\n",
    "args.lr = 1e-3\n",
    "args.num_layers = 17\n",
    "args.num_filters = 64\n",
    "args.crop_size = 256\n",
    "args.debug = False\n",
    "args.test_set = None\n",
    "args.integrate_all_set = True\n",
    "args.individual_noisy_input = True\n",
    "args.wholedataset_version = 'v2'\n",
    "args.x_f_num = 'F01'\n",
    "args.y_f_num = 'F02'\n",
    "args.batch_size = 1\n",
    "args.seed = 0\n",
    "\n",
    "import random\n",
    "# control the randomness\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed_all(args.seed)\n",
    "torch.backends.cudnn.deterministic = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from core.loss_functions import mse_bias, mse_affine, emse_affine, mse_affine_with_tv\n",
    "def determine_loss_and_output_type(args):\n",
    "    if args.loss_function== 'MSE': # upper bound 1-1(optional)\n",
    "        args.loss_fn = torch.nn.MSELoss()\n",
    "        args.num_output_channel = 1\n",
    "        args.output_type = \"linear\"\n",
    "    elif args.loss_function == 'N2V': #lower bound\n",
    "        args.loss_fn = mse_bias\n",
    "        args.num_output_channel = 1\n",
    "        args.output_type = \"linear\"\n",
    "    elif args.loss_function == 'MSE_Affine': # 1st(our case upper bound)\n",
    "        args.loss_fn = mse_affine\n",
    "        args.num_output_channel = 2\n",
    "        args.output_type = \"linear\"\n",
    "    elif args.loss_function == 'EMSE_Affine':\n",
    "        \n",
    "        args.loss_fn = emse_affine\n",
    "        args.num_output_channel = 2\n",
    "    elif args.loss_function == 'MSE_Affine_with_tv':\n",
    "        args.loss_fn = mse_affine_with_tv\n",
    "        vnum_output_channel = 2\n",
    "        args.output_type = \"linear\"\n",
    "    return args\n",
    "def get_X_hat(Z, output):\n",
    "\n",
    "    X_hat = output[:,:1] * Z + output[:,1:]\n",
    "        \n",
    "    #print(X_hat[0])\n",
    "    return X_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set None\n",
      "3600 ['SET05', 'SET06', 'SET07', 'SET08', 'SET09', 'SET10']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Tr loader SET05 F01 vs F02===\n",
      "3600/3600  images are loaded\n",
      "===Tr loader SET06 F01 vs F02===\n",
      "7200/7200  images are loaded\n",
      "===Tr loader SET07 F01 vs F02===\n",
      "10800/10800  images are loaded\n",
      "===Tr loader SET08 F01 vs F02===\n",
      "14400/14400  images are loaded\n",
      "===Tr loader SET09 F01 vs F02===\n",
      "18000/18000  images are loaded\n",
      "===Tr loader SET10 F01 vs F02===\n",
      "21600/21600  images are loaded\n",
      "noisy_arr :  (21600, 256, 256) pixel value range from 0.0 ~ 1.0\n",
      "clean_arr :  (21600, 256, 256) pixel value range from 0.0 ~ 1.0\n",
      "num of training patches :  21600\n",
      "===Te loader SET05 ['F01', 'F02', 'F04', 'F08', 'F16', 'F32']  train on F01 vs F02===\n",
      "===Te loader SET06 ['F01', 'F02', 'F04', 'F08', 'F16', 'F32']  train on F01 vs F02===\n",
      "===Te loader SET07 ['F01', 'F02', 'F04', 'F08', 'F16', 'F32']  train on F01 vs F02===\n",
      "===Te loader SET08 ['F01', 'F02', 'F04', 'F08', 'F16', 'F32']  train on F01 vs F02===\n",
      "===Te loader SET09 ['F01', 'F02', 'F04', 'F08', 'F16', 'F32']  train on F01 vs F02===\n",
      "===Te loader SET10 ['F01', 'F02', 'F04', 'F08', 'F16', 'F32']  train on F01 vs F02===\n",
      "300/300  images are loaded\n",
      "num of test images :  300\n"
     ]
    }
   ],
   "source": [
    "tr_file_path = \"/mnt/ssd/hyun/fbi-net/FBI-Denoiser/data/train_Samsung_SNU_patches_SET050607080910_divided_by_fnum_setnum.hdf5\"\n",
    "tr_data_loader = TrdataLoader(_tr_data_dir=tr_file_path,_args=args)\n",
    "tr_data_loader = DataLoader(tr_data_loader, batch_size=args.batch_size, shuffle=True, num_workers=0, drop_last=True)\n",
    "\n",
    "te_file_path = file_path = \"/mnt/ssd/hyun/fbi-net/FBI-Denoiser/data/test_Samsung_SNU_patches_SET050607080910_divided_by_fnum_setnum.hdf5\"\n",
    "te_data_loader = TedataLoader(_te_data_dir=te_file_path,args=args)\n",
    "te_data_loader = DataLoader(te_data_loader, batch_size=1, shuffle=False, num_workers=0, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "args.loss_function = \"MSE_Affine\"\n",
    "args.nepochs = 20\n",
    "args = determine_loss_and_output_type(args)\n",
    "model = Generator(dim=1, num_output_channel= args.num_output_channel, ngf_factor=3, depth=4).cuda()\n",
    "# from core.model import New_model\n",
    "# model = New_model(channel = 1, output_channel =  args.um_output_channel, \n",
    "#                                 filters = args.num_filters, num_of_layers=args.num_layers, case = args.model_type, \n",
    "#                                 BSN_type = {\"type\" : args.BSN_type, \"param\" : args.BSN_param},\n",
    "#                                 output_type = args.output_type, sigmoid_value = args.sigmoid_value)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchsummary'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/hyunwoong/FBI-Denoiser/denoise_and_get_CD/test_CD_depends_on_epoch/filter_guidance_model.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B147.46.91.55/home/hyunwoong/FBI-Denoiser/denoise_and_get_CD/test_CD_depends_on_epoch/filter_guidance_model.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchsummary\u001b[39;00m \u001b[39mimport\u001b[39;00m summary\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B147.46.91.55/home/hyunwoong/FBI-Denoiser/denoise_and_get_CD/test_CD_depends_on_epoch/filter_guidance_model.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m summary(model, (\u001b[39m1\u001b[39m, \u001b[39m256\u001b[39m, \u001b[39m256\u001b[39m))\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchsummary'"
     ]
    }
   ],
   "source": [
    "# from torchsummary import summary\n",
    "# summary(model, (1, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from core.utils import get_PSNR,get_SSIM\n",
    "def eval(args):\n",
    "    psnr_arr = []\n",
    "    ssim_arr = []\n",
    "    loss_arr = []\n",
    "    \n",
    "    denoised_img_arr = []\n",
    "    with torch.no_grad():\n",
    "         for batch_idx, (source, target) in enumerate(te_data_loader):\n",
    "            source = source.cuda()\n",
    "            target = target.cuda()\n",
    "            \n",
    "            output = model(source)\n",
    "            loss = args.loss_fn(output, target).cpu().numpy()\n",
    "                        \n",
    "            # Update loss\n",
    "            if args.loss_function == 'MSE':\n",
    "                output = output.cpu().numpy()\n",
    "                X_hat = np.clip(output, 0, 1)\n",
    "                X = target.cpu().numpy()\n",
    "                \n",
    "            elif args.loss_function[:10] == 'MSE_Affine':\n",
    "                \n",
    "                Z = target[:,:1]\n",
    "                X = target[:,1:].cpu().numpy()\n",
    "                X_hat = np.clip(get_X_hat(Z,output).cpu().numpy(), 0, 1)\n",
    "                \n",
    "            elif  args.loss_function == 'N2V':\n",
    "                X = target[:,1:].cpu().numpy()\n",
    "                X_hat = np.clip(output.cpu().numpy(), 0, 1)\n",
    "\n",
    "            \n",
    "            loss_arr.append(loss)\n",
    "            psnr_arr.append(get_PSNR(X[0], X_hat[0]))\n",
    "            ssim_arr.append(get_SSIM(X[0], X_hat[0],args.data_type))\n",
    "            denoised_img_arr.append(X_hat[0].reshape(X_hat.shape[2],X_hat.shape[3]))\n",
    "            print(f\"Loss : {loss:.4f}, PSNR : {psnr_arr[-1]:.2f} dB, SSIM : {ssim_arr[-1]:.4f}\")\n",
    "            if batch_idx == 0:\n",
    "                plt.subplot(131)\n",
    "                plt.imshow(source[0], cmap='gray')\n",
    "                plt.subplot(132)\n",
    "                plt.title(f\"PSNR : {psnr_arr[-1]:.2f} dB\\nSSIM : {ssim_arr[-1]:.4f}\")\n",
    "                plt.imshow(target[0], cmap='gray')\n",
    "                plt.subplot(133)\n",
    "                plt.imshow(target[0,1:].cpu().numpy().reshape(X_hat.shape[2],X_hat.shape[3]), cmap='gray')\n",
    "                plt.pause(0.01)\n",
    "            break\n",
    "    mean_loss = np.mean(loss_arr)\n",
    "    mean_psnr = np.mean(psnr_arr)\n",
    "    mean_ssim = np.mean(ssim_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "is_test = True\n",
    "def train(args):\n",
    "    \n",
    "    optim = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "    mean_tr_loss = []\n",
    "    for epoch in range(1,args.nepochs+1):\n",
    "        print(\"====== Epoch %d ======\" % epoch)\n",
    "        tr_loss = []\n",
    "        pbar = tqdm(enumerate(tr_data_loader),total=len(tr_data_loader))\n",
    "        for batch_idx, (source, target) in pbar:\n",
    "\n",
    "            optim.zero_grad()\n",
    "            # if is_test is True:\n",
    "            #     print(source.shape, target.shape)\n",
    "                \n",
    "            source = source.cuda()\n",
    "            target = target.cuda()\n",
    "            \n",
    "            output = model(source)\n",
    "            # print(output.shape)\n",
    "            loss = args.loss_fn(output,target)\n",
    "            # loss = loss + \n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            tr_loss.append(loss.detach().cpu().numpy())\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "        mean_tr_loss.append(np.mean(tr_loss))\n",
    "        \n",
    "        eval(args)\n",
    "    return mean_tr_loss, (source, target, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "mseloss = nn.MSELoss()\n",
    "is_test = True\n",
    "def train_constrained_weight_change(args):\n",
    "    \n",
    "    optim = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "    mean_tr_loss = []\n",
    "    for epoch in range(1,args.nepochs+1):\n",
    "        print(\"====== Epoch %d ======\" % epoch)\n",
    "        tr_loss = []\n",
    "        pbar = tqdm(enumerate(tr_data_loader),total=len(tr_data_loader))\n",
    "        for batch_idx, (src, tgt) in pbar:\n",
    "                \n",
    "            source = src.clone().cuda()\n",
    "            target = tgt.cuda()\n",
    "            \n",
    "            output = model(source)\n",
    "            model_weight = model.state_dict()\n",
    "            # print(output.shape)\n",
    "            loss_1st = args.loss_fn(output,target)\n",
    "            loss_1st.backward(retain_graph=True)\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "            tr_loss.append(loss_1st)\n",
    "            \n",
    "            source = src.clone().cuda()\n",
    "            output_updated = model(source)\n",
    "            output_changed = mseloss(output_updated, output)\n",
    "            loss_2nd = args.loss_fn(output,target) + args.lambda_weight * output_changed\n",
    "            loss_2nd.backward()\n",
    "            model.load_state_dict(model_weight)\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "            \n",
    "            pbar.set_postfix(loss_1st=loss_1st.item(), loss_2nd = loss_2nd.item(),\n",
    "                             output_changed = output_changed.item())\n",
    "        mean_tr_loss.append(np.mean(tr_loss))\n",
    "        \n",
    "        eval(args)\n",
    "    return mean_tr_loss, (source, target, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Epoch 1 ======\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.010547876358032227,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 21600,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2de6ffba5c034dd88d782dbcb38f46e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [2, 3, 1, 1]] is at version 8; expected version 7 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/hyunwoong/FBI-Denoiser/denoise_and_get_CD/test_CD_depends_on_epoch/filter_guidance_model.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B147.46.91.55/home/hyunwoong/FBI-Denoiser/denoise_and_get_CD/test_CD_depends_on_epoch/filter_guidance_model.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# result = train(args)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B147.46.91.55/home/hyunwoong/FBI-Denoiser/denoise_and_get_CD/test_CD_depends_on_epoch/filter_guidance_model.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m args\u001b[39m.\u001b[39mlambda_weight \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B147.46.91.55/home/hyunwoong/FBI-Denoiser/denoise_and_get_CD/test_CD_depends_on_epoch/filter_guidance_model.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m result \u001b[39m=\u001b[39m train_constrained_weight_change(args)\n",
      "\u001b[1;32m/home/hyunwoong/FBI-Denoiser/denoise_and_get_CD/test_CD_depends_on_epoch/filter_guidance_model.ipynb Cell 12\u001b[0m in \u001b[0;36mtrain_constrained_weight_change\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B147.46.91.55/home/hyunwoong/FBI-Denoiser/denoise_and_get_CD/test_CD_depends_on_epoch/filter_guidance_model.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m output_changed \u001b[39m=\u001b[39m mseloss(output_updated, output)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B147.46.91.55/home/hyunwoong/FBI-Denoiser/denoise_and_get_CD/test_CD_depends_on_epoch/filter_guidance_model.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m loss_2nd \u001b[39m=\u001b[39m args\u001b[39m.\u001b[39mloss_fn(output,target) \u001b[39m+\u001b[39m args\u001b[39m.\u001b[39mlambda_weight \u001b[39m*\u001b[39m output_changed\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B147.46.91.55/home/hyunwoong/FBI-Denoiser/denoise_and_get_CD/test_CD_depends_on_epoch/filter_guidance_model.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m loss_2nd\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B147.46.91.55/home/hyunwoong/FBI-Denoiser/denoise_and_get_CD/test_CD_depends_on_epoch/filter_guidance_model.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m model\u001b[39m.\u001b[39mload_state_dict(model_weight)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B147.46.91.55/home/hyunwoong/FBI-Denoiser/denoise_and_get_CD/test_CD_depends_on_epoch/filter_guidance_model.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m optim\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/fbi-net/lib/python3.9/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/fbi-net/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [2, 3, 1, 1]] is at version 8; expected version 7 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "# epoch 5 ~ 7 정도에서 괜찮은 결과\n",
    "# args.lr = 1e-3\n",
    "# args.batch_size = 1\n",
    "# epoch 3~5 정도에서 좋은 결과\n",
    "# args.lr = 1e-3\n",
    "# args.batch_size = 16\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "weights_init(model)\n",
    "args.lr = 1e-4\n",
    "args.batch_size = 16\n",
    "# result = train(args)\n",
    "args.lambda_weight = 1\n",
    "result = train_constrained_weight_change(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
